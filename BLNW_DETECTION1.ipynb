{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e1f1c8-69ea-46b6-8864-825cb36f6fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12186 images belonging to 4 classes.\n",
      "Found 3044 images belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSVV\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSVV\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m704s\u001b[0m 2s/step - accuracy: 0.7606 - loss: 5.1605 - val_accuracy: 0.2500 - val_loss: 75.8761\n",
      "Epoch 2/10\n",
      "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m624s\u001b[0m 2s/step - accuracy: 0.8912 - loss: 0.3209 - val_accuracy: 0.8219 - val_loss: 2.3288\n",
      "Epoch 3/10\n",
      "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m606s\u001b[0m 2s/step - accuracy: 0.9140 - loss: 0.2385 - val_accuracy: 0.6239 - val_loss: 33.2296\n",
      "Epoch 4/10\n",
      "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 2s/step - accuracy: 0.9115 - loss: 0.2250 - val_accuracy: 0.5762 - val_loss: 23.0060\n",
      "Epoch 5/10\n",
      "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1851s\u001b[0m 5s/step - accuracy: 0.9237 - loss: 0.2096 - val_accuracy: 0.7244 - val_loss: 3.8086\n",
      "Epoch 6/10\n",
      "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m607s\u001b[0m 2s/step - accuracy: 0.9411 - loss: 0.1572 - val_accuracy: 0.9350 - val_loss: 1.8088\n",
      "Epoch 7/10\n",
      "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m599s\u001b[0m 2s/step - accuracy: 0.9467 - loss: 0.1566 - val_accuracy: 0.6567 - val_loss: 38.6627\n",
      "Epoch 8/10\n",
      "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m597s\u001b[0m 2s/step - accuracy: 0.9371 - loss: 0.1777 - val_accuracy: 0.6577 - val_loss: 2.9310\n",
      "Epoch 9/10\n",
      "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m646s\u001b[0m 2s/step - accuracy: 0.9550 - loss: 0.1269 - val_accuracy: 0.6810 - val_loss: 86.2447\n",
      "Epoch 10/10\n",
      "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m663s\u001b[0m 2s/step - accuracy: 0.9622 - loss: 0.1075 - val_accuracy: 0.2500 - val_loss: 345.0147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 20:32:59.133 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\MSVV\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import streamlit as st\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "# ---- STEP 1: DATA PREPROCESSING ----\n",
    "dataset_path = r\"D:\\Project\\Bolt ,Nut, project\\blnw-images-224\"\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_data = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "class_names = list(train_data.class_indices.keys())\n",
    "\n",
    "# ---- STEP 2: CNN MODEL ----\n",
    "def create_cnn_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "        \n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "        \n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(len(class_names), activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ---- STEP 3: TRAIN & SAVE MODEL ----\n",
    "model = create_cnn_model()\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "model.save(\"blnw_cnn_model.h5\")\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "# ---- STEP 4: STREAMLIT GUI ----\n",
    "st.title(\"Mechanical Part Detector (Bolt, Nut, Washer, Locating Pin)\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    image = load_img(uploaded_file, target_size=(224, 224))\n",
    "    image_array = img_to_array(image) / 255.0\n",
    "    image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "    # Load Model\n",
    "    model = tf.keras.models.load_model(\"blnw_cnn_model.h5\")\n",
    "\n",
    "    prediction = model.predict(image_array)\n",
    "    predicted_class = class_names[np.argmax(prediction)]\n",
    "\n",
    "    st.image(uploaded_file, caption=f\"Predicted: {predicted_class}\", use_column_width=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b756be-2f64-4664-a99f-76c7497516f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"blnw_cnn_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c674a9a3-a034-476d-b7be-767588c03d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(r\"D:\\blnw_detection\\blnw_cnn_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cae7bc7c-94e4-4d3d-b372-163f0cef9f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "save_path = r\"D:\\blnw_detection\"\n",
    "os.makedirs(save_path, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "model.save(os.path.join(save_path, \"blnw_cnn_model.h5\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95628f48-25f2-462d-b602-d7ee830dfd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import streamlit as st\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from PIL import Image\n",
    "\n",
    "# ---- Load the trained model ----\n",
    "MODEL_PATH = r\"D:\\blnw_detection\\blnw_cnn_model.h5\"  # Update with your saved model\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "# Define class labels (must match dataset)\n",
    "class_names = ['bolt', 'locatingpin', 'nut', 'washer']\n",
    "\n",
    "# ---- Streamlit UI ----\n",
    "st.title(\"Mechanical Parts Detector 🔧\")\n",
    "st.sidebar.title(\"Upload Image or Use Webcam\")\n",
    "choice = st.sidebar.radio(\"Select Input Mode\", (\"Upload Image\", \"Use Webcam\"))\n",
    "\n",
    "def predict_image(image):\n",
    "    \"\"\"Preprocess the image and get predictions.\"\"\"\n",
    "    img_array = img_to_array(image) / 255.0  # Normalize image\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class = class_names[np.argmax(prediction)]\n",
    "    confidence = np.max(prediction) * 100\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# ---- Image Upload Mode ----\n",
    "if choice == \"Upload Image\":\n",
    "    uploaded_file = st.file_uploader(\"Upload an Image\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
    "    if uploaded_file is not None:\n",
    "        image = Image.open(uploaded_file).resize((224, 224))\n",
    "        st.image(image, caption=\"Uploaded Image\", use_container_width=True)\n",
    "        \n",
    "        predicted_class, confidence = predict_image(image)\n",
    "        st.success(f\"Detected: **{predicted_class}** ({confidence:.2f}%)\")\n",
    "\n",
    "# ---- Webcam Mode ----\n",
    "elif choice == \"Use Webcam\":\n",
    "    st.write(\"Starting webcam... Press `Q` to exit.\")\n",
    "\n",
    "    cap = cv2.VideoCapture(0)  # Open laptop camera\n",
    "    frame_placeholder = st.empty()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            st.error(\"Failed to capture video!\")\n",
    "            break\n",
    "\n",
    "        # Convert frame to RGB (Streamlit uses RGB, OpenCV uses BGR)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img_resized = cv2.resize(frame_rgb, (224, 224))\n",
    "\n",
    "        # Predict object\n",
    "        predicted_class, confidence = predict_image(img_resized)\n",
    "\n",
    "        # Convert to grayscale for edge detection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        edges = cv2.Canny(gray, 50, 150)\n",
    "        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        for contour in contours:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            if w > 20 and h > 20:  # Ignore small noise\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                size_text = f\"Size: {w}x{h} px\"\n",
    "                cv2.putText(frame, size_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Show prediction\n",
    "        text = f\"{predicted_class} ({confidence:.2f}%)\"\n",
    "        cv2.putText(frame, text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "\n",
    "        # Display video stream in Streamlit\n",
    "        frame_placeholder.image(frame, channels=\"BGR\")\n",
    "\n",
    "        # Stop when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51df2d81-fe70-48fc-b240-abd7ba9dc51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12186 images belonging to 4 classes.\n",
      "Found 3044 images belonging to 4 classes.\n",
      "Class Names: ['bolt', 'locatingpin', 'nut', 'washer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSVV\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSVV\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 789ms/step - accuracy: 0.7322 - loss: 0.8293 - val_accuracy: 0.8614 - val_loss: 0.6483\n",
      "Epoch 2/5\n",
      "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 774ms/step - accuracy: 0.9615 - loss: 0.1177 - val_accuracy: 0.9185 - val_loss: 0.5131\n",
      "Epoch 3/5\n",
      "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 767ms/step - accuracy: 0.9763 - loss: 0.0634 - val_accuracy: 0.9060 - val_loss: 0.7171\n",
      "Epoch 4/5\n",
      "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 768ms/step - accuracy: 0.9864 - loss: 0.0422 - val_accuracy: 0.8890 - val_loss: 1.0132\n",
      "Epoch 5/5\n",
      "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 772ms/step - accuracy: 0.9888 - loss: 0.0307 - val_accuracy: 0.9014 - val_loss: 0.8600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at D:\\blnw_detection\\blnw_cnn_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import streamlit as st\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# ---- STEP 1: DATA PREPROCESSING ----\n",
    "dataset_path = r\"D:\\Project\\Bolt ,Nut, project\\blnw-images-224\"\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "\n",
    "train_data = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "class_names = list(train_data.class_indices.keys())  # Get class names\n",
    "print(\"Class Names:\", class_names)\n",
    "\n",
    "# Compute class weights to fix imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_data.classes), y=train_data.classes)\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# ---- STEP 2: CNN MODEL CREATION ----\n",
    "def create_cnn_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),\n",
    "        MaxPooling2D(2,2),\n",
    "        \n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        MaxPooling2D(2,2),\n",
    "        \n",
    "        Conv2D(128, (3,3), activation='relu'),\n",
    "        MaxPooling2D(2,2),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(len(class_names), activation='softmax')  # Output layer\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and train model\n",
    "model = create_cnn_model()\n",
    "history = model.fit(train_data, validation_data=val_data, epochs=5, class_weight=class_weights_dict)\n",
    "\n",
    "# ---- STEP 3: SAVE MODEL ----\n",
    "save_path = r\"D:\\blnw_detection\\blnw_cnn_model.h5\"\n",
    "model.save(save_path)\n",
    "print(f\"Model saved at {save_path}\")\n",
    "\n",
    "# ---- STEP 4: STREAMLIT GUI ----\n",
    "st.title(\"Mechanical Parts Detector 🔧\")\n",
    "st.sidebar.title(\"Upload Image or Use Webcam\")\n",
    "choice = st.sidebar.radio(\"Select Input Mode\", (\"Upload Image\", \"Use Webcam\"))\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(save_path)\n",
    "\n",
    "def predict_image(image):\n",
    "    \"\"\"Preprocess the image and get predictions.\"\"\"\n",
    "    image = image.resize((224, 224))  # Resize to match training input\n",
    "    img_array = img_to_array(image)\n",
    "    img_array = img_array / 255.0  # Normalize (same as training)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    \n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class = class_names[np.argmax(prediction)]\n",
    "    confidence = np.max(prediction) * 100\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# ---- Image Upload Mode ----\n",
    "if choice == \"Upload Image\":\n",
    "    uploaded_file = st.file_uploader(\"Upload an Image\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
    "    if uploaded_file is not None:\n",
    "        image = Image.open(uploaded_file).resize((224, 224))\n",
    "        st.image(image, caption=\"Uploaded Image\", use_container_width=True)\n",
    "        \n",
    "        predicted_class, confidence = predict_image(image)\n",
    "        st.success(f\"Detected: **{predicted_class}** ({confidence:.2f}%)\")\n",
    "\n",
    "# ---- Webcam Mode ----\n",
    "elif choice == \"Use Webcam\":\n",
    "    st.write(\"Starting webcam... Press `Q` to exit.\")\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    frame_placeholder = st.empty()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            st.error(\"Failed to capture video!\")\n",
    "            break\n",
    "\n",
    "        # Convert OpenCV BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_pil = Image.fromarray(frame_rgb)  # Convert to PIL Image\n",
    "\n",
    "        # Resize and predict\n",
    "        predicted_class, confidence = predict_image(frame_pil)\n",
    "\n",
    "        # Show results\n",
    "        text = f\"{predicted_class} ({confidence:.2f}%)\"\n",
    "        cv2.putText(frame, text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "\n",
    "        frame_placeholder.image(frame, channels=\"BGR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "971a9d7e-f701-4db8-aa30-f4361810b3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from PIL import Image\n",
    "\n",
    "# Load Trained Model\n",
    "MODEL_PATH = r\"D:\\blnw_detection\\blnw_cnn_model.h5\"\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "# Class Names (should match dataset labels)\n",
    "class_names = ['bolt', 'locatingpin', 'nut', 'washer']\n",
    "\n",
    "def predict_image(image):\n",
    "    \"\"\"Preprocess the image and get predictions.\"\"\"\n",
    "    image = image.resize((224, 224))  # Resize image\n",
    "    img_array = img_to_array(image)\n",
    "    img_array = img_array / 255.0  # Normalize\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    \n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class = class_names[np.argmax(prediction)]\n",
    "    confidence = np.max(prediction) * 100\n",
    "    \n",
    "    return predicted_class, confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdb41053-3a4d-464e-85d9-041411a623bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstreamlit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mst\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpredict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m predict_image  \u001b[38;5;66;03m# Import prediction function\u001b[39;00m\n\u001b[0;32m      6\u001b[0m st\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMechanical Parts Detector 🔧\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m st\u001b[38;5;241m.\u001b[39msidebar\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload Image or Use Webcam\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'predict'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from predict import predict_image  # Import prediction function\n",
    "\n",
    "st.title(\"Mechanical Parts Detector 🔧\")\n",
    "st.sidebar.title(\"Upload Image or Use Webcam\")\n",
    "choice = st.sidebar.radio(\"Select Input Mode\", (\"Upload Image\", \"Use Webcam\"))\n",
    "\n",
    "# ---- IMAGE UPLOAD MODE ----\n",
    "if choice == \"Upload Image\":\n",
    "    uploaded_file = st.file_uploader(\"Upload an Image\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
    "    if uploaded_file is not None:\n",
    "        image = Image.open(uploaded_file).resize((224, 224))\n",
    "        st.image(image, caption=\"Uploaded Image\", use_container_width=True)\n",
    "        \n",
    "        predicted_class, confidence = predict_image(image)\n",
    "        st.success(f\"Detected: **{predicted_class}** ({confidence:.2f}%)\")\n",
    "\n",
    "# ---- WEBCAM MODE ----\n",
    "elif choice == \"Use Webcam\":\n",
    "    st.write(\"Starting webcam... Press `Q` to exit.\")\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    frame_placeholder = st.empty()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            st.error(\"Failed to capture video!\")\n",
    "            break\n",
    "\n",
    "        # Convert OpenCV BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_pil = Image.fromarray(frame_rgb)  # Convert to PIL Image\n",
    "\n",
    "        # Resize and predict\n",
    "        predicted_class, confidence = predict_image(frame_pil)\n",
    "\n",
    "        # Show results\n",
    "        text = f\"{predicted_class} ({confidence:.2f}%)\"\n",
    "        cv2.putText(frame, text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "\n",
    "        frame_placeholder.image(frame, channels=\"BGR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7dd3e29-2f65-4ed0-824c-5765aad504ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "save_path = r\"D:\\BLNW\"\n",
    "os.makedirs(save_path, exist_ok=True)  # Create folder if it doesn't exist\n",
    "\n",
    "model.save(os.path.join(save_path, \"blnw_cnn_model.h5\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eac76a3e-f4a9-4c32-a133-fd219c9e1924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import streamlit as st\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from PIL import Image\n",
    "\n",
    "# ---- Load the trained model ----\n",
    "MODEL_PATH = r\"D:\\BLNW\\blnw_cnn_model.h5\"  # Ensure this path is correct\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "# Define class labels (must match dataset)\n",
    "class_names = ['bolt', 'locatingpin', 'nut', 'washer']\n",
    "\n",
    "# ---- Streamlit UI ----\n",
    "st.title(\"Mechanical Parts Detector 🔧\")\n",
    "st.sidebar.title(\"Upload Image or Use Webcam\")\n",
    "choice = st.sidebar.radio(\"Select Input Mode\", (\"Upload Image\", \"Use Webcam\"))\n",
    "\n",
    "def predict_image(image):\n",
    "    \"\"\"Preprocess the image and get predictions.\"\"\"\n",
    "    img_array = img_to_array(image)\n",
    "    img_array = cv2.resize(img_array, (224, 224))  # Ensure correct input size\n",
    "    img_array = preprocess_input(img_array)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class = class_names[np.argmax(prediction)]\n",
    "    confidence = np.max(prediction) * 100\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# ---- Image Upload Mode ----\n",
    "if choice == \"Upload Image\":\n",
    "    uploaded_file = st.file_uploader(\"Upload an Image\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
    "    if uploaded_file is not None:\n",
    "        image = Image.open(uploaded_file).convert(\"RGB\")\n",
    "        image = image.resize((224, 224))  # Ensure resizing\n",
    "        st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
    "        \n",
    "        predicted_class, confidence = predict_image(image)\n",
    "        st.success(f\"Detected: **{predicted_class}** ({confidence:.2f}%)\")\n",
    "\n",
    "# ---- Webcam Mode ----\n",
    "elif choice == \"Use Webcam\":\n",
    "    st.write(\"Starting webcam... Press `Q` to exit.\")\n",
    "\n",
    "    cap = cv2.VideoCapture(0)  # Open laptop camera\n",
    "    frame_placeholder = st.empty()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            st.error(\"Failed to capture video!\")\n",
    "            break\n",
    "\n",
    "        # Convert frame to RGB (Streamlit uses RGB, OpenCV uses BGR)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img_resized = cv2.resize(frame_rgb, (224, 224))\n",
    "\n",
    "        # Predict object\n",
    "        predicted_class, confidence = predict_image(img_resized)\n",
    "\n",
    "        # Draw bounding box around detected parts\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        edges = cv2.Canny(gray, 50, 150)\n",
    "        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        for contour in contours:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            if w > 20 and h > 20:  # Ignore small noise\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                size_text = f\"Size: {w}x{h} px\"\n",
    "                cv2.putText(frame, size_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Show prediction\n",
    "        text = f\"{predicted_class} ({confidence:.2f}%)\"\n",
    "        cv2.putText(frame, text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "\n",
    "        # Display video stream in Streamlit\n",
    "        frame_placeholder.image(frame, channels=\"BGR\")\n",
    "\n",
    "        # Stop when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2cc3049-21a1-4658-9ee9-12896eba8379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5672b556-c3f7-44de-9988-5030fb95d5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"blnw_cnn_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b955b4a-0e2f-417f-90fe-78d926d9c530",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get true labels and predicted labels\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_true \u001b[38;5;241m=\u001b[39m \u001b[43mval_data\u001b[49m\u001b[38;5;241m.\u001b[39mclasses  \u001b[38;5;66;03m# True labels\u001b[39;00m\n\u001b[0;32m      3\u001b[0m y_pred_probs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(val_data)  \u001b[38;5;66;03m# Get predicted probabilities\u001b[39;00m\n\u001b[0;32m      4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_pred_probs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Convert to class labels\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Get true labels and predicted labels\n",
    "y_true = val_data.classes  # True labels\n",
    "y_pred_probs = model.predict(val_data)  # Get predicted probabilities\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # Convert to class labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4c96fe6-3b2d-4b68-9b57-63055eaaa745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3044 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define dataset path and image size\n",
    "dataset_path = r\"blnw-images-224\"\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Reload validation dataset\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False  # Important: No shuffling to align predictions with true labels\n",
    ")\n",
    "\n",
    "class_names = list(val_data.class_indices.keys())  # Get class names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9ef0dc-5f2f-429c-bcb1-53a138c9cf8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
